{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n","\n","if False:\n","    import os\n","    # Turn off GPU\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n","    gpu = False\n","else:\n","    gpu = True\n","\n","from tensorflow import keras\n","from copy import deepcopy\n","import tensorflow as tf\n","from glob import glob\n","import numpy as np\n","import os\n","\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","\n","print(gpus)\n","\n","if gpu:\n","    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024 * 3))])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from glob import glob\n","\n","models = sorted(glob(\"model-saved\\\\*2023\\\\*[!.dict]\"))\n","models[:5], len(models)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import data_utils\n","\n","DATA_DIR = \"utils/split/_split_/_type_\"\n","IMG_SIZE = (256, 256)\n","train_val_X = sorted(glob(DATA_DIR.replace(\"_split_\", \"train-val\").replace(\"_type_\", \"img\") + \"\\\\*\"))\n","train_val_Y = sorted(glob(DATA_DIR.replace(\"_split_\", \"train-val\").replace(\"_type_\", \"mask\") + \"\\\\*\"))\n","\n","# test_X = sorted(glob(DATA_DIR.replace(\"_split_\", \"test\").replace(\"_type_\", \"img\") + \"\\\\*\"))\n","# test_Y = sorted(glob(DATA_DIR.replace(\"_split_\", \"test\").replace(\"_type_\", \"mask\") + \"\\\\*\"))\n","\n","\n","# test_data = data_utils.load_testset(test_X, test_Y, IMAGE_SIZE=IMG_SIZE, BATCH_SIZE=1, REMAP=\"binary\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","from sklearn.metrics import ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","def confusion_matrix(y_true, y_pred):\n","    # True positive 1 & 1\n","    TP = np.bitwise_and(\n","        (y_true == 1), (y_pred == 1)).sum()\n","    # False positive 0 & 1\n","    FP = np.bitwise_and(\n","        (y_true == 0), (y_pred == 1)).sum()\n","    # True negative 0 & 0\n","    TN = np.bitwise_and(\n","        (y_true == 0), (y_pred == 0)).sum()\n","    # False negative 1 & 0\n","    FN = np.bitwise_and(\n","        (y_true == 1), (y_pred == 0)).sum()\n","\n","    output = np.array([[TN, FP], [FN, TP]])\n","\n","    return output, output.sum()\n","\n","def run_confusion_matrix(model, dataset, normalize=False):\n","    BATCH = IMG = 0\n","    MASK = 1\n","\n","    matrix = np.array([[0, 0], [0, 0]])\n","    total = 0\n","\n","    for data in dataset:\n","        y_true = np.reshape(data[MASK][BATCH], (256, 256)) \\\n","            .flatten()\n","        y_pred = model.predict(data[IMG], verbose=0, workers=100)[BATCH] \\\n","            .astype(np.uint8).flatten()\n","\n","        matrixPartial, totalPartial = confusion_matrix(y_true, y_pred)\n","\n","        matrix += matrixPartial\n","        total += totalPartial\n","\n","    if normalize:\n","        matrix = matrix / total\n","    \n","    return matrix\n","\n","def mIOU(y_true, y_pred):\n","    ulabels = np.unique(y_true)[:-1]\n","\n","    iou = np.zeros(len(ulabels))\n","\n","    for k, u in enumerate(ulabels):\n","        inter = (y_true == u) & (y_pred == u)\n","        union = (y_true == u) | (y_pred == u)\n","\n","        iou[k] = inter.sum() / union.sum()\n","\n","    return iou.mean()\n","\n","def run_mIOU(model, dataset):\n","    BATCH = IMG = 0\n","    MASK = 1\n","    \n","    mIOUList = []\n","\n","    for data in dataset:\n","        y_true = np.reshape(data[MASK][BATCH], (256, 256)) \\\n","            .flatten()\n","        y_pred = model.predict(data[IMG], verbose=0, workers=100)[BATCH] \\\n","            .astype(np.uint8).flatten()\n","\n","        mIOUList.append(mIOU(y_true, y_pred))\n","    return np.array(mIOUList).round(5)\n","\n","class plot_distribution():\n","\n","    def __init__(self, plot_name):\n","        self.plot_name = plot_name\n","        self.data = {}\n","\n","    def add_plot(self, name, data):\n","        self.data[name] = data\n","\n","    def show_plot(self, save=False):\n","        fig, ax = plt.subplots()\n","        ax.set_title(self.plot_name)\n","        \n","        sns.set_context(\"paper\")\n","        \n","        ax.set_xlabel(\"mIOU\")\n","        ax.set_ylabel(\"mIOU frequency\")\n","\n","        ax = sns.histplot(data=self.data, legend=True, kde=True, \n","                          common_norm=False, stat=\"count\", ax=ax,\n","                          palette=sns.color_palette(n_colors=5))\n","\n","        if save:\n","            fig.savefig(\"plot/\"+self.plot_name+\".svg\", format=\"svg\")\n","        \n","        self.data = {}\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outlier_dict = {}\n","mIOU_dict = {}\n","import json\n","from models import deeplabV3\n","IMG_SIZE = (256, 256)\n","\n","for modelPath in models:\n","    train_val_data = data_utils.load_dataset(train_val_X, train_val_Y, \n","                                             IMAGE_SIZE=IMG_SIZE, BATCH_SIZE=1,\n","                                             REMAP=\"binary\", N_FOLDS=5, SEED=42)\n","            \n","    for fold, (trainDataset, valDataset, n_classes) in enumerate(train_val_data):\n","        # model-saved\\\\01-03-2023\\\\deeplabv3+_1_lr_0.001_alpha_0.5_256x256_layer_11_binary_Skip\n","        modelName = modelPath.split(\"\\\\\")[-1]\n","\n","        modelFold = int(modelName.split(\"_\")[1])\n","        modelLr = modelName.split(\"_\")[3]\n","        modelAlpha = float(modelName.split(\"_\")[5])\n","        modelLayer = modelName.split(\"_\")[8]\n","\n","        if (fold != modelFold):\n","            continue\n","\n","        mobileLayers = {\"shallowLayer\": \"block_2_project_BN\",\n","                    \"deepLayer\": f\"block_{modelLayer}_project_BN\"}\n","\n","        model = deeplabV3(imageSize=IMG_SIZE, nClasses=n_classes, alpha=modelAlpha, \n","                            withArgmax=True, mobileLayers=mobileLayers)\n","        model.load_weights(modelPath)\n","\n","        tempList = run_mIOU(model, valDataset)\n","\n","        tempDict = {\n","            \"Name\":modelName,\n","            \"Mean\": np.mean(tempList).round(5),\n","            \"Variance\": np.var(tempList).round(5),\n","            \"StandardDeviation\": np.std(tempList).round(5),\n","            \"Median\": np.median(tempList),\n","            \"IoU_ValSet\": tempList\n","        }\n","\n","        print(f\"{tempDict['Name']} | Mean {tempDict['Mean']} | Variance {tempDict['Variance']} | Standard Deviation {tempDict['StandardDeviation']}\")\n","\n","        try:\n","            mIOU_dict[f\"alpha_{modelAlpha}_layer_{modelLayer}\"] += [tempDict]\n","        except (KeyError):\n","            mIOU_dict[f\"alpha_{modelAlpha}_layer_{modelLayer}\"] = [tempDict]\n","\n","        tempDict = tempList = None\n","\n","# for key in mIOU_dict.keys():\n","#     with open(key + '.json', 'w') as fout:\n","#         json.dump({key:mIOU_dict[key]}, fout)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outlier_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","\n","newDict = {}\n","for key in mIOU_dict.keys():\n","    newDict[key] = []\n","    for dicts in mIOU_dict[key]:\n","        keys = list(dicts.keys())\n","        newDict[key].append({\n","            keys[0]:{\n","                keys[1]: dicts[keys[1]],\n","                keys[2]: dicts[keys[2]],\n","                keys[3]: dicts[keys[3]],\n","                \"IoU\" : dicts[keys[0]]}})\n","        \n","for key in newDict.keys():\n","    with open(key + '.json', 'w') as fout:\n","        json.dump({key:newDict[key]}, fout)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from glob import glob\n","\n","files = sorted(glob(\"*.json\"))\n","files[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","\n","i = 0\n","\n","metricDict = {}\n","\n","for file in files:\n","    with open(file, 'r') as fin:\n","        # read json with a model cross validation and \n","        data = json.load(fin)\n","\n","        split = list(data.keys())[0]\n","        \n","        auxDict = {}\n","\n","        # remove list of IoU\n","        for key in data[split].keys():\n","            # del data[split][key][\"mIOU_ValSet\"]\n","            data[split][key][\"name\"] = key\n","\n","            fold = \"fold \" + key.split(\"_\")[1]\n","\n","            auxDict[fold] = data[split][key]\n","\n","        metricDict[split] = auxDict\n","\n","len(metricDict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list(metricDict.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getOutliers(tempList):\n","    if not isinstance(tempList, np.array.__class__):\n","        tempList = np.array(tempList)\n","\n","    q1, q3 = np.quantile(tempList, [.25, .75])\n","\n","    outlierLess = q1 - 1.5 * (q3-q1)\n","    outlierMore = q3 + 1.5 * (q3-q1)\n","\n","    outlier_index = np.logical_or(tempList <= outlierLess, tempList >= outlierMore)\n","\n","    outlierList = tempList[outlier_index]\n","\n","    return outlierList, outlier_index"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import display\n","import pandas as pd\n","\n","keys = list(metricDict.keys())\n","\n","for key in keys:\n","    key = key.split(\"_\")\n","\n","    split = f\"alpha_{key[1]}_layer_{key[3]}\"\n","\n","    plotOBJ = plot_distribution(split.replace(\"_\", \" \"))\n","\n","    dictAux = {}\n","    print(split)\n","    for fold in sorted(metricDict[split].keys()):\n","        mIOUValues = np.array(metricDict[split][fold][\"mIOU_ValSet\"])\n","\n","        outlierList, outlier_index = getOutliers(mIOUValues)\n","        lenOutliers = len(outlierList)\n","        \n","\n","        dictAux[fold] = {\n","            \"Mean\": np.mean(mIOUValues),\n","            \"StandardDeviation\": np.std(mIOUValues),\n","            \"Outliers\": lenOutliers}\n","\n","        # mIOUValues = mIOUValues[~outlier_index]\n","\n","        plotOBJ.add_plot(fold, mIOUValues)\n","\n","    df = pd.DataFrame(dictAux).transpose()\n","    display(df)\n","\n","    plotOBJ.show_plot(save=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mIOUList = run_mIOU(model, valDataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_distribution(mIOU_dict['alpha_0.5_layer_11'][0]['deeplabv3+_1_lr_0.001_alpha_0.5_256x256_layer_11_binary_Skip'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["matrix = run_confusion_matrix(model, valDataset, normalize=True)\n","print(matrix)\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=[\"False\", \"True\"])\n","disp.plot()\n","disp.ax_.set(xlabel='Predicted', ylabel='Groundtruth')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["matrix = run_confusion_matrix(model, test_data, normalize=False)\n","print(matrix)\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=[\"False\", \"True\"])\n","disp.plot()\n","disp.ax_.set(xlabel='Predicted', ylabel='Groundtruth')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"tf_gpu","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
